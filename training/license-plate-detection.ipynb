{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10964018,"sourceType":"datasetVersion","datasetId":6821371,"isSourceIdPinned":false},{"sourceId":10964565,"sourceType":"datasetVersion","datasetId":6821786,"isSourceIdPinned":false},{"sourceId":10965251,"sourceType":"datasetVersion","datasetId":6822247,"isSourceIdPinned":false},{"sourceId":10984039,"sourceType":"datasetVersion","datasetId":6836118},{"sourceId":11019325,"sourceType":"datasetVersion","datasetId":6861441}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport cv2\nimport shutil\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:11.772246Z","iopub.execute_input":"2025-03-13T14:40:11.772547Z","iopub.status.idle":"2025-03-13T14:40:12.410815Z","shell.execute_reply.started":"2025-03-13T14:40:11.772519Z","shell.execute_reply":"2025-03-13T14:40:12.410084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\nprint(f'{torch.cuda.is_available() = }')\nprint(f'{torch.cuda.device_count() = }')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:12.411844Z","iopub.execute_input":"2025-03-13T14:40:12.412291Z","iopub.status.idle":"2025-03-13T14:40:15.812755Z","shell.execute_reply.started":"2025-03-13T14:40:12.412261Z","shell.execute_reply":"2025-03-13T14:40:15.811812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing dataset\n","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/*\nwith open (\"/kaggle/input/training-dataset/GreenParking/location.txt\", \"r\") as file:\n    lines = file.readlines()\n# print(lines[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:15.814581Z","iopub.execute_input":"2025-03-13T14:40:15.814952Z","iopub.status.idle":"2025-03-13T14:40:15.962914Z","shell.execute_reply.started":"2025-03-13T14:40:15.814930Z","shell.execute_reply":"2025-03-13T14:40:15.961974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path_dataset = \"/kaggle/working/dataset\"\nos.mkdir(path_dataset)\nlabels_data = \"/kaggle/working/dataset/labels\"\nimages_data = \"/kaggle/working/dataset/images\"\nos.mkdir(labels_data); os.mkdir(images_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:15.964242Z","iopub.execute_input":"2025-03-13T14:40:15.964462Z","iopub.status.idle":"2025-03-13T14:40:15.968801Z","shell.execute_reply.started":"2025-03-13T14:40:15.964442Z","shell.execute_reply":"2025-03-13T14:40:15.968021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Processing motobike plate from: training-dataset (over 1700 images)","metadata":{}},{"cell_type":"code","source":"# labels\ninput_path = \"/kaggle/input/training-dataset/GreenParking\"\nwith open (\"/kaggle/input/training-dataset/GreenParking/location.txt\", \"r\") as file:\n    lines = file.readlines()\nfor line in lines:\n    line = line.strip().split()\n    img_name = line[0]\n    line[2:] = map(float, line[2:])\n    img = cv2.imread(os.path.join(input_path, img_name))\n    height, width, _ = img.shape\n    # format bounding box for yolov12\n    x_center = round((line[2] + line[4]/2)/width, 6) #x_center\n    y_center = round((line[3] + line[5]/2)/height, 6) #y_center\n    wid = round(line[4]/width, 6)   #width \n    hei =round(line[5]/height, 6)  #height\n    # label = \" \".join(line[1:])\n    label_path = os.path.join(labels_data, img_name.replace(\".jpg\", \".txt\"))\n    with open(label_path, \"w\") as file:\n        file.write(f'{0} {x_center} {y_center} {wid} {hei}\\n' )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:15.969812Z","iopub.execute_input":"2025-03-13T14:40:15.970093Z","iopub.status.idle":"2025-03-13T14:40:31.951987Z","shell.execute_reply.started":"2025-03-13T14:40:15.970064Z","shell.execute_reply":"2025-03-13T14:40:31.951056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# images\nfrom distutils.dir_util import copy_tree\ncopy_tree(\"/kaggle/input/training-dataset/GreenParking\", images_data)\nos.remove(\"/kaggle/working/dataset/images/location.txt\")\ncopy_tree(\"/kaggle/input/number-plate-dataset-3/images\", images_data)\ncopy_tree(\"/kaggle/input/number-plate-dataset-3/labels\", labels_data)\nos.remove(\"/kaggle/working/dataset/images/2.1.png\")\nos.remove(\"/kaggle/working/dataset/images/1.1.PNG\")\nos.remove(\"/kaggle/working/dataset/labels/2.1.txt\")\nos.remove(\"/kaggle/working/dataset/labels/1.1.txt\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:31.953080Z","iopub.execute_input":"2025-03-13T14:40:31.953384Z","iopub.status.idle":"2025-03-13T14:40:41.950626Z","shell.execute_reply.started":"2025-03-13T14:40:31.953354Z","shell.execute_reply":"2025-03-13T14:40:41.949975Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Processing car plate from: car-plate-dataset (over 1000 images)","metadata":{}},{"cell_type":"code","source":"input_path = \"/kaggle/input/car-plate/car-plate\"\ndef rename(name_file):\n    name, ext = os.path.splitext(name_file)\n    name = name.split(\".rf.\")[0]\n    return name+ext\n    \ndef copy_file(src, dst):\n    for name_file in os.listdir(src):\n        shutil.copy(os.path.join(src, name_file), os.path.join(dst, rename(name_file)))\n\ncopy_file(\"/kaggle/input/car-plate/car-plate/images\", images_data)\ncopy_file(\"/kaggle/input/car-plate/car-plate/labels\", labels_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:41.951339Z","iopub.execute_input":"2025-03-13T14:40:41.951552Z","iopub.status.idle":"2025-03-13T14:40:52.948144Z","shell.execute_reply.started":"2025-03-13T14:40:41.951532Z","shell.execute_reply":"2025-03-13T14:40:52.947390Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split data","metadata":{}},{"cell_type":"code","source":"label_dir = \"/kaggle/working/dataset/labels\"\nlist_labels = [f for f in os.listdir(label_dir) if f.endswith(\".txt\")]\nfor label in list_labels:\n    label_path = os.path.join(label_dir, label)\n    with open(label_path, \"r\") as f:\n        lines = f.readlines()\n    new_lines = []\n    for line in lines:\n        values = line.strip().split() \n        bbox = list(map(float, values[1:])) \n        bbox[0] = max(bbox[2] / 2, bbox[0])  # X-center >= w/2\n        bbox[1] = max(bbox[3] / 2, bbox[1])  # Y_center >= h/2\n        new_lines.append(f\"0 {' '.join(map(str, bbox))}\\n\")\n\n    with open(label_path, \"w\") as f:\n        f.writelines(new_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:52.950227Z","iopub.execute_input":"2025-03-13T14:40:52.950447Z","iopub.status.idle":"2025-03-13T14:40:53.271181Z","shell.execute_reply.started":"2025-03-13T14:40:52.950429Z","shell.execute_reply":"2025-03-13T14:40:53.270460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Gets list of labels and images name\nlist_images = [f for f in os.listdir(\"/kaggle/working/dataset/images\")]\nlist_labels = [f for f in os.listdir(\"/kaggle/working/dataset/labels\")]\n\nassert len(list_images) == len(list_labels)\n\n# train, val: 80/20\ntrain, val = train_test_split(list_images, test_size=1/5, random_state=42)\n\nprint(f'''\n      len(train) = {len(train)}\n      len(val) = {len(val)}\n''')\n# print(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:53.272406Z","iopub.execute_input":"2025-03-13T14:40:53.272608Z","iopub.status.idle":"2025-03-13T14:40:53.887208Z","shell.execute_reply.started":"2025-03-13T14:40:53.272590Z","shell.execute_reply":"2025-03-13T14:40:53.886377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.mkdir(\"data_training\")\ndef move_data(list_data, dst):\n    os.mkdir(dst)\n    images_dst = os.path.join(dst, \"images\"); os.mkdir(images_dst)\n    labels_dst = os.path.join(dst, \"labels\"); os.mkdir(labels_dst)\n    for image_name in list_data:\n        image_path = os.path.join(\"/kaggle/working/dataset/images\", image_name)\n        shutil.copy(image_path, os.path.join(images_dst, image_name))\n        label_name = image_name.replace(\".jpg\", \".txt\")\n        label_path = os.path.join(\"/kaggle/working/dataset/labels\", label_name)\n        shutil.copy(label_path, os.path.join(labels_dst, label_name))\n\nmove_data(train, \"/kaggle/working/data_training/train\")\nmove_data(val, \"/kaggle/working/data_training/val\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:53.888010Z","iopub.execute_input":"2025-03-13T14:40:53.888413Z","iopub.status.idle":"2025-03-13T14:40:54.757397Z","shell.execute_reply.started":"2025-03-13T14:40:53.888375Z","shell.execute_reply":"2025-03-13T14:40:54.756731Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Data Augmentation","metadata":{}},{"cell_type":"code","source":"!pip install albumentations==1.3.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:54.758211Z","iopub.execute_input":"2025-03-13T14:40:54.758440Z","iopub.status.idle":"2025-03-13T14:40:59.589459Z","shell.execute_reply.started":"2025-03-13T14:40:54.758420Z","shell.execute_reply":"2025-03-13T14:40:59.588577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom tqdm import tqdm\n\ntransform = A.Compose([\n    A.RandomBrightnessContrast(p=0.3),  \n    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n    A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.3),\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=30, p=0.3),\n    A.MotionBlur(blur_limit=5, p=0.3),\n    A.GaussNoise(p=0.3),  \n    A.Perspective(scale=(0.05, 0.15), p=0.3),\n    A.RandomSizedBBoxSafeCrop(height=256, width=256, p=0.3),\n    A.MedianBlur(blur_limit=3, p=0.3),\n    A.RandomRain(drop_length=5, drop_width=1, blur_value=3, p=0.3),\n    A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.3),  \n    A.RandomShadow(shadow_roi=(0, 0.5, 1, 1), num_shadows_lower=1, num_shadows_upper=2, p=0.3), \n], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n\ndef read_yolo_labels(label_path):\n    with open(label_path, \"r\") as file:\n        lines = file.readlines()\n    bboxes = []\n    for line in lines:\n        parts = line.strip().split()\n        bbox = list(map(float, parts[1:])) \n        bboxes.append(bbox)\n    return bboxes\n\ndef save_yolo_labels(label_path, bboxes):\n    with open(label_path, \"w\") as file:\n        for bbox in bboxes:\n            file.write(f\"0 \" + \" \".join(map(str, bbox)) + \"\\n\")\n        \ndef augment_data(image_folder, label_folder, num_aug=4):\n    image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\")]\n    for img_file in tqdm(image_files):\n        img_path = os.path.join(image_folder, img_file)\n        label_path = os.path.join(label_folder, img_file.replace(\".jpg\", \".txt\"))\n        image = cv2.imread(img_path)\n        h, w, _ = image.shape\n        bboxes = read_yolo_labels(label_path)\n\n        # Create augmented\n        for i in range(num_aug):\n            # print(bboxes)\n            class_labels = [\"0\"] * len(bboxes) \n            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n            aug_img = augmented[\"image\"]\n            aug_bboxes = augmented[\"bboxes\"]\n            # print(aug_bboxes)\n\n            aug_img_filename = f\"{img_file.replace('.jpg', '')}_aug_{i}.jpg\"\n            aug_label_filename = f\"{img_file.replace('.jpg', '')}_aug_{i}.txt\"\n            cv2.imwrite(os.path.join(image_folder, aug_img_filename), aug_img)\n            save_yolo_labels(os.path.join(label_folder, aug_label_filename), aug_bboxes)\n\nimage_folder = \"/kaggle/working/data_training/train/images\"\nlabel_folder =  \"/kaggle/working/data_training/train/labels\"\naugment_data(image_folder, label_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:40:59.590471Z","iopub.execute_input":"2025-03-13T14:40:59.590784Z","iopub.status.idle":"2025-03-13T14:48:48.223174Z","shell.execute_reply.started":"2025-03-13T14:40:59.590758Z","shell.execute_reply":"2025-03-13T14:48:48.222347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Check dataset by creating folder contains: images are drawed labels","metadata":{}},{"cell_type":"code","source":"image_dir = '/kaggle/working/data_training/train/images'\nlabel_dir = '/kaggle/working/data_training/train/labels'\nlabel_img_dir = '/kaggle/working/data_training/labeled_image'\nos.makedirs(label_img_dir)\n\nfor img_name in tqdm(os.listdir(image_dir)):\n    img_path = os.path.join(image_dir, img_name)\n    image = cv2.imread(img_path)\n    # image = image[:, :, ::-1]\n    # plt.imshow(image)\n\n    label_path = os.path.join(label_dir, img_name.replace('.jpg', '.txt'))\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n\n    # Plot the bounding box on the image\n    for line in lines:\n        class_id, x_center, y_center, width, height = map(float, line.strip().split())\n        img_height, img_width, _ = image.shape\n        \n        # print(class_id, x_center, y_center, width, height, img_height, img_width)\n        x_center *= img_width\n        y_center *= img_height\n        width *= img_width\n        height *= img_height\n        x1 = int(x_center - width / 2)\n        y1 = int(y_center - height / 2)\n        x2 = int(x_center + width / 2)\n        y2 = int(y_center + height / 2)\n        # print((x1, y1), (x2, y2))\n        # Draw bounding box\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        cv2.imwrite(os.path.join(label_img_dir, img_name), image)\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:48:48.224014Z","iopub.execute_input":"2025-03-13T14:48:48.224657Z","iopub.status.idle":"2025-03-13T14:49:44.796472Z","shell.execute_reply.started":"2025-03-13T14:48:48.224632Z","shell.execute_reply":"2025-03-13T14:49:44.795492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !zip -r /kaggle/working/data_training.zip /kaggle/working/data_training\n!rm -rf /kaggle/working/data_training\n!rm -rf /kaggle/working/dataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:28:25.632680Z","iopub.execute_input":"2025-03-13T15:28:25.632998Z","iopub.status.idle":"2025-03-13T15:28:27.349378Z","shell.execute_reply.started":"2025-03-13T15:28:25.632970Z","shell.execute_reply":"2025-03-13T15:28:27.348158Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Define datasets.yaml file\nimport yaml\n\ndatasets_yaml = {\n    'path':'/kaggle/working/data_training',\n    'train': 'train',\n    'val': 'val',\n    'nc': 1, \n    'names': ['0'],\n}\n\nwith open('datasets.yaml', 'w') as file:\n    yaml.dump(datasets_yaml, file)\n\n# rm /kaggle/working/datasets.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.904871Z","iopub.status.idle":"2025-03-13T14:49:44.905116Z","shell.execute_reply":"2025-03-13T14:49:44.905014Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"!pip install -U ultralytics \n!pip install -U ipywidgets -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.905897Z","iopub.status.idle":"2025-03-13T14:49:44.906249Z","shell.execute_reply":"2025-03-13T14:49:44.906099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shutil.rmtree(\"/kaggle/working/runs\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.906849Z","iopub.status.idle":"2025-03-13T14:49:44.907208Z","shell.execute_reply":"2025-03-13T14:49:44.907044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\n# model = YOLO('yolo12n.pt')\nmodel = YOLO(\"/kaggle/input/model/best.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.908865Z","iopub.status.idle":"2025-03-13T14:49:44.909237Z","shell.execute_reply":"2025-03-13T14:49:44.909069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.909812Z","iopub.status.idle":"2025-03-13T14:49:44.910157Z","shell.execute_reply":"2025-03-13T14:49:44.909999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.train(\n#     data='datasets.yaml',  \n#     epochs=100,           \n#     batch=16,             \n#     device='cuda',         \n#     imgsz=640,  \n#     iou = 0.75,\n#     # workers=4,\n#     # cache=True            \n#     # iou - lr0 - momentum - dropout \n# )","metadata":{"trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.910705Z","iopub.status.idle":"2025-03-13T14:49:44.911117Z","shell.execute_reply":"2025-03-13T14:49:44.910948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the trained model\nmodel.save('best_model.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.911673Z","iopub.status.idle":"2025-03-13T14:49:44.912281Z","shell.execute_reply":"2025-03-13T14:49:44.912114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the training results\nresults = pd.read_csv(os.path.join(\"/kaggle/working/runs/detect/train/\", 'results.csv'))\nresults.columns = results.columns.str.strip()  \n\n# Extract epochs and accuracy metrics\nepochs = results.index + 1  \nmAP_0_5 = results['metrics/mAP50(B)']  # Mean Average Precision at IoU=0.5\nmAP_0_5_0_95 = results['metrics/mAP50-95(B)']  # MAP at IoU=0.5:0.95\n\nplt.figure(figsize=(10, 5))\nplt.plot(epochs, mAP_0_5, label='mAP_0.5')\nplt.plot(epochs, mAP_0_5_0_95, label='mAP_0.5:0.95')\nplt.xlabel('Epoch');   plt.ylabel('Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.legend(loc=\"lower right\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.915010Z","iopub.status.idle":"2025-03-13T14:49:44.915392Z","shell.execute_reply":"2025-03-13T14:49:44.915223Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prediction and Text Extraction Use Paddle OCR","metadata":{}},{"cell_type":"code","source":"!pip install paddleocr\n# !pip uninstall paddlepaddle-gpu\n!pip install paddlepaddle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.916454Z","iopub.status.idle":"2025-03-13T14:49:44.916871Z","shell.execute_reply":"2025-03-13T14:49:44.916680Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !rm -rf /kaggle/working/results\nr = [[[[[162.0, 60.0], [193.0, 69.0], [189.0, 80.0], [158.0, 70.0]], \n       ('TaJ]m-~P\\\\L6*XMV5<O_-}:IPL0LE0c0%Dv8i2:&1', 1702.5013427734375)],\n      [[[264.0, 65.0], [285.0, 61.0], [288.0, 73.0], [266.0, 77.0]], \n       ('i\\\\EtFV*:,90&g3n7:A6Z>^3#(2P  B*', 914.424072265625)], \n      [[[515.0, 172.0], [528.0, 172.0], [528.0, 178.0], [515.0, 178.0]], \n       (' J+~H\\'-&I\"h}r|^|py\\\\orsv0o=. -3hkpiZ0uN#', 1416.0770263671875)]]]\nr[0][2]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.918106Z","iopub.status.idle":"2025-03-13T14:49:44.918469Z","shell.execute_reply":"2025-03-13T14:49:44.918309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from paddleocr import PaddleOCR, draw_ocr\nimport cv2\nimport re\ntest_folder = '/kaggle/working/results'\n!rm -rf /kaggle/working/results\nos.mkdir(test_folder)\nCONFIDENCE_THRESHOLD = 0.5\n\n# https://paddlepaddle.github.io/PaddleOCR/latest/en/quick_start.html#use-by-code\nocr = PaddleOCR(use_angle_cls = True, use_gpu = False)\n\ndef paddle_ocr(image, x1, y1, x2, y2):\n    image = image[y1:y2, x1: x2]\n    result = ocr.ocr(image, det=False, rec = True, cls = False)\n    text = \"\"\n    for r in result:\n        #print(\"OCR\", r)\n        scores = r[0][1]\n        if np.isnan(scores):\n            scores = 0\n        else:  # threshold\n            scores = int(scores * 100)\n        if scores > 60:\n            text = r[0][0]\n    pattern = re.compile('[\\W]')\n    text = pattern.sub('', text)\n    text = text.replace(\"???\", \"\")\n    text = text.replace(\"O\", \"0\")\n    text = text.replace(\"ç²¤\", \"\")\n    return str(text)\n\ndef predict_ocr(image):\n    results = model.predict(image, device='cpu')\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # plt.imshow(image)\n        \n    for result in results:\n        filtered_boxes = [box for box in result.boxes if box.conf[0] > CONFIDENCE_THRESHOLD]\n        for box in filtered_boxes:\n            x1, y1, x2, y2 = map(int, box.xyxy[0])\n            confidence = box.conf[0]  # Get confidence\n            \n            # Crop bounding box\n            # rect = image[y1:y2, x1:x2]\n            # plt.imshow(rect)\n            # print((x1, y1), (x2, y2))\n            text = paddle_ocr(image, x1, y1, x2, y2)\n            # print(\"NONE\") if result==None else print(result)\n            # text = result[0][0]\n                    \n            print(f\"Detected text: {confidence:.2f}_{text}\")\n            cv2.putText(image, f'{confidence:.2f}_{text}', (x1-30, y1-10), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n                    \n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 3)\n\n            # boxes = [elements[0] for elements in line for line in result]\n            # txts = [elements[1][0] for elements in line for line in result]\n            # scores = [elements[1][1] for elements in line for line in result]\n            # output = draw_ocr(image, boxes, txts, scores, font_path='path_to_font.ttf')\n            \n            # # text = pytesseract.image_to_string(rect, config='--psm 6')  # perform OCR\n    return image\n\ndef predict_folder(path_test):\n    for img in os.listdir(path_test):\n        image = cv2.imread(os.path.join(path_test, img))\n        image = predict_ocr(image)\n        cv2.imwrite(os.path.join(test_folder, img), image)\n        \n# predict_folder('/kaggle/input/demo-ttt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.920112Z","iopub.status.idle":"2025-03-13T14:49:44.920513Z","shell.execute_reply":"2025-03-13T14:49:44.920343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Capture frame from video and predict","metadata":{}},{"cell_type":"code","source":"import cv2\ncount = 0\n!rm -rf /kaggle/working/vid_results\nos.mkdir(\"/kaggle/working/vid_results\")\nwhile True:\n    vidcap = cv2.VideoCapture('/kaggle/input/data-vid-vietnam/vid2.mp4')\n    # fps = vidcap.get(cv2.CAP_PROP_FPS)  # Get FPS of vid\n    # frame_skip = int(fps * 1)   # 1s\n    # vidcap.set(cv2.CAP_PROP_POS_MSEC, time_ms)\n    success, frame = vidcap.read()\n    if success:\n        count += 1\n        if count%30==0:\n            frame = predict_ocr(frame) \n            cv2.imshow(\"Video\", frame)\n            cv2.imwrite(os.path.join(\"/kaggle/working/vid_results\", f\"frame_{count}.jpg\"), frame)\n            cv2.waitKey()\n            cv2.destroyAllWindows()\n            \n        if count == 1000:\n            break\n    else:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:49:44.921132Z","iopub.status.idle":"2025-03-13T14:49:44.921517Z","shell.execute_reply":"2025-03-13T14:49:44.921343Z"}},"outputs":[],"execution_count":null}]}